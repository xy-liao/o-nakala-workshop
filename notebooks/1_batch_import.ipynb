{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Batch Import\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "\n",
    "Create datasets and collections on NAKALA from CSV files.\n",
    "\n",
    "## ðŸ“‹ What This Notebook Does\n",
    "\n",
    "1. Reads `folder_data_items.csv` and `folder_collections.csv`\n",
    "2. Uploads files to NAKALA\n",
    "3. Creates datasets with metadata\n",
    "4. Creates collections linking datasets\n",
    "5. **Auto-generates** CSVs for modification and deletion\n",
    "\n",
    "## ðŸ”„ Workflow\n",
    "\n",
    "```\n",
    "Input CSVs â†’ Upload Files â†’ Create Datasets â†’ Create Collections â†’ Generate CSVs for Next Steps\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path to import nakala package\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Import required libraries\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict\n",
    "\n",
    "# Import from nakala package\n",
    "from nakala import CsvConverter, API_URL, API_KEY\n",
    "from nakala.api_client import upload_file as upload_file_to_nakala\n",
    "from nakala.api_client import create_dataset as create_nakala_dataset\n",
    "from nakala.api_client import create_collection as create_nakala_collection\n",
    "\n",
    "print(\"âœ“ Imports successful\")\n",
    "print(f\"âœ“ API URL: {API_URL}\")\n",
    "print(f\"âœ“ Using test API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "BASE_PATH = Path.cwd().parent\n",
    "DATA_PATH = BASE_PATH / 'data'\n",
    "FILES_DIR = BASE_PATH / 'files'\n",
    "\n",
    "# Input CSVs\n",
    "DATASETS_CSV = DATA_PATH / 'folder_data_items.csv'\n",
    "COLLECTIONS_CSV = DATA_PATH / 'folder_collections.csv'\n",
    "\n",
    "# Output CSVs\n",
    "OUTPUT_DATASETS_CSV = DATA_PATH / 'output_datasets.csv'\n",
    "OUTPUT_COLLECTIONS_CSV = DATA_PATH / 'output_collections.csv'\n",
    "\n",
    "print(f\"âœ“ Base path: {BASE_PATH}\")\n",
    "print(f\"âœ“ Data path: {DATA_PATH}\")\n",
    "print(f\"âœ“ Files directory: {FILES_DIR}\")\n",
    "print(f\"\\nâœ“ Input CSVs:\")\n",
    "print(f\"  - {DATASETS_CSV.name}: {'âœ“ exists' if DATASETS_CSV.exists() else 'âœ— missing'}\")\n",
    "print(f\"  - {COLLECTIONS_CSV.name}: {'âœ“ exists' if COLLECTIONS_CSV.exists() else 'âœ— missing'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Preview Input CSVs\n",
    "\n",
    "Let's see what we're going to import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview datasets CSV\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASETS TO IMPORT (folder_data_items.csv)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "with open(DATASETS_CSV, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for i, row in enumerate(reader, 1):\n",
    "        print(f\"\\nDataset {i}:\")\n",
    "        print(f\"  Title: {row.get('title', 'N/A')[:60]}...\")\n",
    "        print(f\"  Files: {row.get('file', 'N/A')}\")\n",
    "        print(f\"  Type: {row.get('type', 'N/A')}\")\n",
    "        print(f\"  Status: {row.get('status', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview collections CSV\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COLLECTIONS TO CREATE (folder_collections.csv)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "with open(COLLECTIONS_CSV, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for i, row in enumerate(reader, 1):\n",
    "        print(f\"\\nCollection {i}:\")\n",
    "        print(f\"  Title: {row.get('title', 'N/A')[:60]}...\")\n",
    "        print(f\"  Data items: {row.get('data_items', 'N/A')}\")\n",
    "        print(f\"  Status: {row.get('status', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Import Datasets\n",
    "\n",
    "This will:\n",
    "1. Upload files to NAKALA\n",
    "2. Create datasets with metadata\n",
    "3. Save results to `output_datasets.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_datasets(csv_path: Path, base_path: Path, api_key: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Import datasets from CSV file\n",
    "    Returns: Dictionary mapping folder paths to dataset IDs\n",
    "    \"\"\"\n",
    "    converter = CsvConverter()\n",
    "    dataset_map = {}  # folder_path -> dataset_id\n",
    "\n",
    "    # Prepare output CSV\n",
    "    output = open(OUTPUT_DATASETS_CSV, 'w', encoding='utf-8')\n",
    "    output_writer = csv.writer(output)\n",
    "    output_writer.writerow(['dataset_id', 'files', 'title', 'folder_path', 'status', 'response'])\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STARTING DATASET IMPORT\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        for row_num, row in enumerate(reader, 1):\n",
    "            try:\n",
    "                print(f\"\\n{'=' * 80}\")\n",
    "                print(f\"ROW {row_num}: {row.get('title', 'Untitled')[:50]}\")\n",
    "                print(f\"{'=' * 80}\")\n",
    "\n",
    "                output_data = ['', '', row.get('title', ''), row.get('file', ''), '', '']\n",
    "\n",
    "                # Parse file paths\n",
    "                folder_path = row.get('file', '')\n",
    "                file_paths = converter.parse_files(folder_path, base_path)\n",
    "\n",
    "                if not file_paths:\n",
    "                    print(f\"âš  No files found for: {folder_path}\")\n",
    "                    output_data[4] = 'ERROR'\n",
    "                    output_data[5] = 'No files found'\n",
    "                    output_writer.writerow(output_data)\n",
    "                    continue\n",
    "\n",
    "                print(f\"âœ“ Found {len(file_paths)} file(s)\")\n",
    "\n",
    "                # Upload files\n",
    "                files_metadata = []\n",
    "                output_files = []\n",
    "\n",
    "                for file_path in file_paths:\n",
    "                    print(f\"  Uploading: {file_path.name}\")\n",
    "                    file_info = upload_file_to_nakala(file_path, api_key)\n",
    "                    if file_info:\n",
    "                        files_metadata.append(file_info)\n",
    "                        output_files.append(f\"{file_path.name},{file_info['sha1']}\")\n",
    "                        print(f\"    âœ“ Uploaded: {file_info['sha1'][:16]}...\")\n",
    "                    else:\n",
    "                        output_files.append(file_path.name)\n",
    "                        output_data[1] = ';'.join(output_files)\n",
    "                        output_data[4] = 'ERROR'\n",
    "                        output_data[5] = f'Failed to upload: {file_path.name}'\n",
    "                        output_writer.writerow(output_data)\n",
    "                        print(f\"    âœ— Upload failed\")\n",
    "                        break\n",
    "\n",
    "                # Check if all files uploaded successfully\n",
    "                if len(files_metadata) != len(file_paths):\n",
    "                    continue\n",
    "\n",
    "                output_data[1] = ';'.join(output_files)\n",
    "\n",
    "                # Convert CSV row to NAKALA metadata\n",
    "                metas = converter.csv_row_to_nakala_metas(row)\n",
    "\n",
    "                # Build dataset JSON\n",
    "                dataset = {\n",
    "                    'status': row.get('status', 'pending').strip(),\n",
    "                    'files': files_metadata,\n",
    "                    'metas': metas\n",
    "                }\n",
    "\n",
    "                print(f\"âœ“ Dataset JSON prepared ({len(metas)} metadata objects)\")\n",
    "\n",
    "                # Create dataset on NAKALA\n",
    "                print(\"  Creating dataset on NAKALA...\")\n",
    "                response = create_nakala_dataset(dataset, api_key)\n",
    "\n",
    "                if response.status_code == 201:\n",
    "                    parsed = response.json()\n",
    "                    dataset_id = parsed['payload']['id']\n",
    "                    print(f\"  âœ“ Dataset created successfully: {dataset_id}\")\n",
    "\n",
    "                    output_data[0] = dataset_id\n",
    "                    output_data[4] = 'OK'\n",
    "                    output_data[5] = response.text\n",
    "\n",
    "                    # Store mapping\n",
    "                    dataset_map[folder_path] = dataset_id\n",
    "\n",
    "                else:\n",
    "                    print(f\"  âœ— Dataset creation failed: {response.status_code}\")\n",
    "                    print(f\"  Response: {response.text}\")\n",
    "                    output_data[4] = 'ERROR'\n",
    "                    output_data[5] = response.text\n",
    "\n",
    "                output_writer.writerow(output_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âœ— Error processing row {row_num}: {str(e)}\")\n",
    "                output_data[4] = 'ERROR'\n",
    "                output_data[5] = str(e)\n",
    "                output_writer.writerow(output_data)\n",
    "\n",
    "    output.close()\n",
    "    print(f\"\\nâœ“ Dataset import complete. Results saved to: {OUTPUT_DATASETS_CSV.name}\")\n",
    "\n",
    "    return dataset_map\n",
    "\n",
    "# Execute import\n",
    "dataset_map = import_datasets(DATASETS_CSV, BASE_PATH, API_KEY)\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"SUMMARY: Created {len(dataset_map)} datasets\")\n",
    "print(f\"{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Import Collections\n",
    "\n",
    "This will:\n",
    "1. Create collections with metadata\n",
    "2. Link datasets to collections\n",
    "3. Save results to `output_collections.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_collections(csv_path: Path, dataset_map: Dict[str, str], api_key: str):\n",
    "    \"\"\"\n",
    "    Import collections from CSV file\n",
    "    \"\"\"\n",
    "    converter = CsvConverter()\n",
    "\n",
    "    # Prepare output CSV\n",
    "    output = open(OUTPUT_COLLECTIONS_CSV, 'w', encoding='utf-8')\n",
    "    output_writer = csv.writer(output)\n",
    "    output_writer.writerow(['collection_id', 'title', 'datasets', 'status', 'response'])\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STARTING COLLECTION IMPORT\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        for row_num, row in enumerate(reader, 1):\n",
    "            try:\n",
    "                print(f\"\\n{'=' * 80}\")\n",
    "                print(f\"COLLECTION {row_num}: {row.get('title', 'Untitled')[:50]}\")\n",
    "                print(f\"{'=' * 80}\")\n",
    "\n",
    "                output_data = ['', row.get('title', ''), '', '', '']\n",
    "\n",
    "                # Convert CSV row to NAKALA metadata\n",
    "                metas = []\n",
    "\n",
    "                # Title (multilingual)\n",
    "                if row.get('title'):\n",
    "                    lang_parts = converter.parse_multilingual_field(row['title'])\n",
    "                    for part in lang_parts:\n",
    "                        meta = {\n",
    "                            \"propertyUri\": converter.property_uris['title'],\n",
    "                            \"value\": part['value']\n",
    "                        }\n",
    "                        if part['lang']:\n",
    "                            meta['lang'] = part['lang']\n",
    "                        metas.append(meta)\n",
    "\n",
    "                # Description (multilingual)\n",
    "                if row.get('description'):\n",
    "                    lang_parts = converter.parse_multilingual_field(row['description'])\n",
    "                    for part in lang_parts:\n",
    "                        meta = {\n",
    "                            \"propertyUri\": converter.property_uris['description'],\n",
    "                            \"value\": part['value'],\n",
    "                            \"typeUri\": \"http://www.w3.org/2001/XMLSchema#string\"\n",
    "                        }\n",
    "                        if part['lang']:\n",
    "                            meta['lang'] = part['lang']\n",
    "                        metas.append(meta)\n",
    "\n",
    "                # Keywords\n",
    "                if row.get('keywords'):\n",
    "                    lang_parts = converter.parse_multilingual_field(row['keywords'])\n",
    "                    for part in lang_parts:\n",
    "                        keywords = converter.parse_multiple_values(part['value'])\n",
    "                        for keyword in keywords:\n",
    "                            meta = {\n",
    "                                \"propertyUri\": converter.property_uris['subject'],\n",
    "                                \"value\": keyword,\n",
    "                                \"typeUri\": \"http://www.w3.org/2001/XMLSchema#string\"\n",
    "                            }\n",
    "                            if part['lang']:\n",
    "                                meta['lang'] = part['lang']\n",
    "                            metas.append(meta)\n",
    "\n",
    "                # Language\n",
    "                if row.get('language'):\n",
    "                    metas.append({\n",
    "                        \"propertyUri\": converter.property_uris['language'],\n",
    "                        \"value\": row['language'].strip(),\n",
    "                        \"typeUri\": \"http://www.w3.org/2001/XMLSchema#string\"\n",
    "                    })\n",
    "\n",
    "                # Creators\n",
    "                if row.get('creator'):\n",
    "                    creators = converter.parse_creator(row['creator'])\n",
    "                    metas.extend(creators)\n",
    "\n",
    "                # Map folder paths to dataset IDs\n",
    "                data_items_str = row.get('data_items', '')\n",
    "                dataset_ids = []\n",
    "\n",
    "                if data_items_str:\n",
    "                    folder_paths = [p.strip() for p in data_items_str.split('|') if p.strip()]\n",
    "                    for folder_path in folder_paths:\n",
    "                        if folder_path in dataset_map:\n",
    "                            dataset_ids.append(dataset_map[folder_path])\n",
    "                            print(f\"  âœ“ Linked dataset: {dataset_map[folder_path]} ({folder_path})\")\n",
    "                        else:\n",
    "                            print(f\"  âš  Dataset not found for folder: {folder_path}\")\n",
    "\n",
    "                output_data[2] = ' | '.join(dataset_ids)\n",
    "\n",
    "                # Build collection JSON\n",
    "                collection = {\n",
    "                    'status': row.get('status', 'private').strip(),\n",
    "                    'metas': metas\n",
    "                }\n",
    "\n",
    "                # Add datasets if any were found\n",
    "                if dataset_ids:\n",
    "                    collection['datas'] = dataset_ids\n",
    "\n",
    "                print(f\"âœ“ Collection JSON prepared ({len(metas)} metadata objects, {len(dataset_ids)} datasets)\")\n",
    "\n",
    "                # Create collection on NAKALA\n",
    "                print(\"  Creating collection on NAKALA...\")\n",
    "                response = create_nakala_collection(collection, api_key)\n",
    "\n",
    "                if response.status_code == 201:\n",
    "                    parsed = response.json()\n",
    "                    collection_id = parsed['payload']['id']\n",
    "                    print(f\"  âœ“ Collection created successfully: {collection_id}\")\n",
    "\n",
    "                    output_data[0] = collection_id\n",
    "                    output_data[3] = 'OK'\n",
    "                    output_data[4] = response.text\n",
    "\n",
    "                else:\n",
    "                    print(f\"  âœ— Collection creation failed: {response.status_code}\")\n",
    "                    print(f\"  Response: {response.text}\")\n",
    "                    output_data[3] = 'ERROR'\n",
    "                    output_data[4] = response.text\n",
    "\n",
    "                output_writer.writerow(output_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âœ— Error processing collection {row_num}: {str(e)}\")\n",
    "                output_data[3] = 'ERROR'\n",
    "                output_data[4] = str(e)\n",
    "                output_writer.writerow(output_data)\n",
    "\n",
    "    output.close()\n",
    "    print(f\"\\nâœ“ Collection import complete. Results saved to: {OUTPUT_COLLECTIONS_CSV.name}\")\n",
    "\n",
    "# Execute import\n",
    "if dataset_map:\n",
    "    import_collections(COLLECTIONS_CSV, dataset_map, API_KEY)\n",
    "else:\n",
    "    print(\"âš  No datasets created, skipping collection import\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate CSVs for Next Steps\n",
    "\n",
    "This will auto-generate:\n",
    "- `modification_data_items.csv` - For modifying datasets (Notebook 2)\n",
    "- `modification_collections.csv` - For modifying collections (Notebook 2)\n",
    "- `delete_data_items.csv` - For deleting datasets (Notebook 3)\n",
    "- `delete_collections.csv` - For deleting collections (Notebook 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_downstream_csvs():\n",
    "    \"\"\"\n",
    "    Update modification and deletion CSVs with IDs from successful imports.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"GENERATING CSVs FOR NEXT STEPS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Parse successful imports from output CSVs\n",
    "    dataset_ids = []\n",
    "    collection_ids = []\n",
    "\n",
    "    # Read dataset IDs from output\n",
    "    if OUTPUT_DATASETS_CSV.exists():\n",
    "        with open(OUTPUT_DATASETS_CSV, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                if row.get('status') == 'OK' and row.get('dataset_id'):\n",
    "                    dataset_ids.append({\n",
    "                        'dataset_id': row['dataset_id'],\n",
    "                        'folder_path': row['folder_path'],\n",
    "                        'title': row['title']\n",
    "                    })\n",
    "\n",
    "    # Read collection IDs from output\n",
    "    if OUTPUT_COLLECTIONS_CSV.exists():\n",
    "        with open(OUTPUT_COLLECTIONS_CSV, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                if row.get('status') == 'OK' and row.get('collection_id'):\n",
    "                    collection_ids.append({\n",
    "                        'collection_id': row['collection_id'],\n",
    "                        'title': row['title']\n",
    "                    })\n",
    "\n",
    "    print(f\"âœ“ Found {len(dataset_ids)} successfully imported datasets\")\n",
    "    print(f\"âœ“ Found {len(collection_ids)} successfully imported collections\")\n",
    "\n",
    "    if not dataset_ids and not collection_ids:\n",
    "        print(\"âš  No successful imports found, skipping CSV generation\")\n",
    "        return\n",
    "\n",
    "    # Read original metadata from folder CSVs\n",
    "    folder_datasets = {}\n",
    "    if DATASETS_CSV.exists():\n",
    "        with open(DATASETS_CSV, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                folder_path = row.get('file', row.get('folder_name', ''))\n",
    "                if not folder_path.endswith('/'):\n",
    "                    folder_path = folder_path + '/'\n",
    "                folder_datasets[folder_path] = row\n",
    "\n",
    "    folder_collections = {}\n",
    "    if COLLECTIONS_CSV.exists():\n",
    "        with open(COLLECTIONS_CSV, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                folder_collections[row.get('title', '')] = row\n",
    "\n",
    "    # Generate modification_data_items.csv\n",
    "    if dataset_ids:\n",
    "        mod_datasets_path = DATA_PATH / 'modification_data_items.csv'\n",
    "        with open(mod_datasets_path, 'w', encoding='utf-8', newline='') as f:\n",
    "            fieldnames = ['dataset_id', 'title', 'creator', 'date', 'license', 'type', 'description', 'keywords', 'status']\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_NONNUMERIC)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for item in dataset_ids:\n",
    "                dataset_id = item['dataset_id']\n",
    "                folder_path = item['folder_path']\n",
    "                original = folder_datasets.get(folder_path, {})\n",
    "\n",
    "                title = original.get('title', item.get('title', ''))\n",
    "                description = original.get('description', '')\n",
    "\n",
    "                writer.writerow({\n",
    "                    'dataset_id': dataset_id,\n",
    "                    'title': title.replace('|', ' v2|') if '|' in title else title + ' v2',\n",
    "                    'creator': original.get('creator', ''),\n",
    "                    'date': original.get('date', ''),\n",
    "                    'license': original.get('license', ''),\n",
    "                    'type': original.get('type', ''),\n",
    "                    'description': description.replace('|', ' (updated)|') if '|' in description else description + ' (updated)',\n",
    "                    'keywords': original.get('keywords', ''),\n",
    "                    'status': 'pending'\n",
    "                })\n",
    "\n",
    "        print(f\"âœ“ Generated: {mod_datasets_path.name}\")\n",
    "\n",
    "    # Generate modification_collections.csv\n",
    "    if collection_ids:\n",
    "        mod_collections_path = DATA_PATH / 'modification_collections.csv'\n",
    "        with open(mod_collections_path, 'w', encoding='utf-8', newline='') as f:\n",
    "            fieldnames = ['collection_id', 'title', 'description', 'keywords', 'status']\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_NONNUMERIC)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for item in collection_ids:\n",
    "                collection_id = item['collection_id']\n",
    "                title = item['title']\n",
    "                original = folder_collections.get(title, {})\n",
    "                description = original.get('description', '')\n",
    "\n",
    "                writer.writerow({\n",
    "                    'collection_id': collection_id,\n",
    "                    'title': title.replace('|', ' v2|') if '|' in title else title + ' v2',\n",
    "                    'description': description.replace('|', ' (updated)|') if '|' in description else description + ' (updated)',\n",
    "                    'keywords': original.get('keywords', ''),\n",
    "                    'status': 'private'\n",
    "                })\n",
    "\n",
    "        print(f\"âœ“ Generated: {mod_collections_path.name}\")\n",
    "\n",
    "    # Generate delete_data_items.csv\n",
    "    if dataset_ids:\n",
    "        del_datasets_path = DATA_PATH / 'delete_data_items.csv'\n",
    "        with open(del_datasets_path, 'w', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['dataset_id', 'confirm_delete'])\n",
    "            for item in dataset_ids:\n",
    "                writer.writerow([item['dataset_id'], 'YES'])\n",
    "\n",
    "        print(f\"âœ“ Generated: {del_datasets_path.name}\")\n",
    "\n",
    "    # Generate delete_collections.csv\n",
    "    if collection_ids:\n",
    "        del_collections_path = DATA_PATH / 'delete_collections.csv'\n",
    "        with open(del_collections_path, 'w', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['collection_id', 'confirm_delete'])\n",
    "            for item in collection_ids:\n",
    "                writer.writerow([item['collection_id'], 'YES'])\n",
    "\n",
    "        print(f\"âœ“ Generated: {del_collections_path.name}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CSV GENERATION COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Execute CSV generation\n",
    "update_downstream_csvs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "\n",
    "### What Was Created\n",
    "\n",
    "Check the `data/` directory for:\n",
    "- `output_datasets.csv` - Import results for datasets\n",
    "- `output_collections.csv` - Import results for collections\n",
    "- `modification_data_items.csv` - **Ready for Notebook 2**\n",
    "- `modification_collections.csv` - **Ready for Notebook 2**\n",
    "- `delete_data_items.csv` - **Ready for Notebook 3**\n",
    "- `delete_collections.csv` - **Ready for Notebook 3**\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Review** the generated `modification_*.csv` files\n",
    "2. **Edit** them if you want to change metadata (e.g., modify titles, descriptions)\n",
    "3. **Run** `2_batch_modify.ipynb` to apply your changes\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: The generated CSVs have \"v2\" and \"(updated)\" markers as examples. You can edit these to any values you want before running Notebook 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}